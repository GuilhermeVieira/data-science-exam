{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAC0459/MAC05865 - Data Science and Engineering \n",
    "\n",
    "Name: Guilherme Costa Vieira\n",
    "NUSP: 9790930"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision trees exercises\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1. What is the approximate depth of a Decision Tree trained (without restrictions) on a training set with 10 million instances?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since most decision tree algorithms consider binary decision trees, we could say the approximate depth is ceil(log2(10 000 000)) = 24 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2. Is a nodeâ€™s Gini impurity generally lower or greater than its parent? Is it generally lower (greater), or always lower (greater)?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us show that the Gini impurity is generally lower than its parent. The Gini impurity is defined as:\n",
    "\n",
    "$$ i(N) = \\sum\\limits_{i \\neq j} P(W_{i}|N)P(w_{j}|N) = \\frac{1}{2} \\left[1 - \\sum\\limits_{j}P^2(W_{j}|N)\\right] $$\n",
    "\n",
    "Define:\n",
    "\n",
    "$P(W|N) = \\frac{w}{n}$ where $w = \\#W$ and $n = \\#N$\n",
    "\n",
    "Define $N_{p}$ and $N_{c}$ as the parent node and the child node respectively, $d_{i} = w_{i} - u_{i}$, $d = \\sum\\limits_{i} d_{i}$ and $d > 0, n > 1$.\n",
    "\n",
    "$$i(N_{p}) = \\frac{1}{2} \\left[1 - \\sum\\limits_{j}\\left(\\frac{w_{j}}{n}\\right)^2\\right] = \\frac{1}{2} \\left[1 - \\frac{\\sum\\limits_{j} w_{j}}{n^2}\\right]\n",
    "$$\n",
    "\n",
    "$$i(N_{c}) = \\frac{1}{2} \\left[1 - \\sum\\limits_{j}\\left(\\frac{u_{j}}{n - d}\\right)^2\\right] = \\frac{1}{2} \\left[1 - \\sum\\limits_{j}\\left(\\frac{w_{j} - d_{j}}{n - d}\\right)^2\\right] =  \\frac{1}{2} \\left[1 - \\frac{ \\sum\\limits_{j}x_{j}^2 - 2\\sum\\limits_{j}x_{j}\\sum\\limits_{j}d_{j} + \\sum\\limits_{j}d_{j}^2}{n^2 - 2nd + d^2}\\right] = \\frac{1}{2} \\left[1 - \\frac{ \\sum\\limits_{j}w_{j}^2 - 2nd + \\sum\\limits_{j}d_{j}^2}{n^2 - 2nd + d^2}\\right]\n",
    "$$\n",
    "\n",
    "Now, we need to show that $i(N_{p}) - i(N_{c}) > 0$:\n",
    "\n",
    "$$i(N_{p}) - i(N_{c}) = \\frac{1}{2} \\left[1 - \\frac{\\sum\\limits_{j} w_{j}}{n^2}\\right] - \\frac{1}{2} \\left[1 - \\frac{ \\sum\\limits_{j}w_{j}^2 - 2nd + \\sum\\limits_{j}d_{j}^2}{n^2 - 2nd + d^2}\\right] = \\frac{1}{2} \\left[1 - \\frac{\\sum\\limits_{j} w_{j}}{n^2} - \\left(1 - \\frac{ \\sum\\limits_{j}w_{j}^2 - 2nd + \\sum\\limits_{j}d_{j}^2}{n^2 - 2nd + d^2}\\right) \\right] = \\frac{1}{2} \\left[-\\frac{\\sum\\limits_{j} w_{j}}{n^2} + \\frac{ \\sum\\limits_{j}w_{j}^2 - 2nd + \\sum\\limits_{j}d_{j}^2}{n^2 - 2nd + d^2}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3. If a Decision Tree is overfitting the training set, is it a good idea to try decreasing maxdepth? Why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, when we decrease the maxdepth we are also reducing the information gain that the last depth would have gained. Hence this new decision tree is less specialized in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4. If a Decision Tree is underfitting the training set, is it a good idea to try scaling the input features? Why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, scaling the input features is good when our classifier relies on Eucliadean distance. The decision trees relies on information gain to make the splits and the information gain measurents make use of probabilities, not Eucliadean distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5. If it takes one hour to train a Decision Tree on a training set containing 10 million instances, roughly how much time will it take to train another Decision Tree on a similar training set containing 100 million instances?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/tree.html#complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6.** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
